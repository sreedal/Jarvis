{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commands to start Development Environment**\n",
    "1. Install Docker-Desktop, Login\n",
    "2. docker run (Jupyter notebook - should update command once loggeing out)\n",
    "    1. docker run --name devenv -it --entrypoint=/bin/bash -p 8880:8888 -v C:\\users\\srmenon\\Documents:/notebooks jupyter/tensorflow-notebook\n",
    "    \n",
    "3. cd /notebooks/\n",
    "4. nohup jupyter notebook --port 8880 --allow-root &\n",
    "5. jupyter notebook list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install twitter\n",
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Twitter & search\n",
    "# https://pypi.org/project/twitter/\n",
    "# https://developer.twitter.com/en/apps/17019807\n",
    "from twitter import *\n",
    "t = Twitter(auth=OAuth(consumer_key=\"nDwKHutvaskyGecZkCS1SiYY8\", consumer_secret=\"zlQdJIuFUGGglbMO2kYMk6oNLFztzipxhRANNz5q5gqfn1tGb1\", token=\"73568030-GqfoqkfWRhqhARS9uzz51vg1UGSV7AGRaOxDkAurZ\", token_secret=\"1kiHGfyDfO7gcaJbGFnr3lnLA3vXc8ti9hJhj1lnTTvEm\"))\n",
    "t.search.tweets(q=\"$MSFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download latest info about ticker from Y! Finance Feed \n",
    "# https://query1.finance.yahoo.com/v7/finance/chart/MSFT?&interval=5m\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "json.load(urlopen(\"https://query1.finance.yahoo.com/v7/finance/chart/MSFT?&interval=5m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feedparser to download and parse reuters feed for business news\n",
    "# https://www.pythonforbeginners.com/feedparser/using-feedparser-in-python\n",
    "# http://feeds.reuters.com/reuters/businessNews\n",
    "import feedparser\n",
    "d = feedparser.parse(\"http://feeds.reuters.com/reuters/businessNews\")\n",
    "print([l.title for l in d[\"entries\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commands to start Kafka Cluster**\n",
    "\n",
    "https://success.docker.com/article/getting-started-with-kafka\n",
    "\n",
    "Use linux container mode (else bridging will fail for overlay network)\n",
    "\n",
    "1. docker swarm init\n",
    "2. docker network create -d overlay --attachable kafka-net\n",
    "3. docker service create --network kafka-net --name=zookeeper  --publish 2181:2181 qnib/plain-zookeeper:latest\n",
    "4. docker service create --network kafka-net --name=zkui --publish 9090:9090 qnib/plain-zkui:latest\n",
    "5. Visit http://localhost:9090/  # Login using admin/manager\n",
    "6. docker service create --network kafka-net --name broker --publish 9092:9092 --hostname=\"{{.Service.Name}}.{{.Task.Slot}}.{{.Task.ID}}\" -e KAFKA_BROKER_ID={{.Task.Slot}} -e ZK_SERVERS=tasks.zookeeper qnib/plain-kafka:latest\n",
    "7. docker ps -q --filter \"label=com.docker.swarm.service.name=broker\"\n",
    "8. docker exec -t -e JMX_PORT=\"\" f6dc03b145c0 /opt/kafka/bin/kafka-topics.sh --zookeeper tasks.zookeeper:2181 --partitions=1 --replication-factor=1 --create --topic test\n",
    "9. docker service create --network kafka-net --name manager -e ZOOKEEPER_HOSTS=tasks.zookeeper --publish=9000:9000 qnib/plain-kafka-manager:latest\n",
    "\n",
    "http://localhost:9000/clusters/kafka\n",
    "\n",
    "OR EQUIVALENTLY\n",
    "\n",
    "https://docs.docker.com/compose/gettingstarted/\n",
    "\n",
    "1. docker-compose up -d KafkaSetup.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'test',\n",
    "     bootstrap_servers=['broker.1.5ftnqdo6u8ee4888cazael8df:9092'],\n",
    "     api_version= (0,11),\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     #group_id='my-group',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "for message in consumer:\n",
    "    message = message.value\n",
    "    print('{0} added'.format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['broker.1.5ftnqdo6u8ee4888cazael8df:9092'],\n",
    "    api_version= (0,11),\n",
    "    value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "for e in range(4):\n",
    "    print(\"Processing \"+str(e)+\"\\n\")\n",
    "    data = {'number' : e}\n",
    "    producer.send('test', value=data)\n",
    "    sleep(1)\n",
    "    \n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Troubleshooting the network**\n",
    "\n",
    "https://success.docker.com/article/troubleshooting-container-networking\n",
    "\n",
    "1. docker run -it --network container:devenv nicolaka/netshoot\n",
    "2. lookup broker.1.c3xailkxvbttt0cdph1n303i1 [Successful]\n",
    "3. nc -zvw2 @backend_ip <listening_port>\n",
    "4. nslookup <IP_address>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Troubleshooting the network\n",
    "\n",
    "import socket\n",
    "#print(socket.gethostbyname('www.google.com'))\n",
    "#print(socket.gethostbyname('0410cdf71fcc')) #Succeeds now (add notebook container to the kafka-net network)\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "try:\n",
    "    sock.connect(('10.0.1.73',9092))\n",
    "except Exception as e:\n",
    "    print(\"something's wrong with %s:%d. Exception is %s\" % (address, port, e))\n",
    "finally:\n",
    "    sock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySPARK Begins Here**\n",
    "\n",
    "1. docker run -d -p 8888:8888 -v C:\\Users\\srmenon\\Documents:/home/jovyan/work --name spark jupyter/pyspark-notebook\n",
    "2. docker container ls\n",
    "3. docker exec 8602565b15e7 jupyter notebook list\n",
    "4. docker network connect kafka-net spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing spark\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a consumer group to use the spark context\n",
    "\n",
    "1. docker exec -t -e JMX_PORT=\"\" 8cbb69e65d13 /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server broker.1.5ftnqdo6u8ee4888cazael8df:9092 --topic test --consumer-property groupd.id=test-consumer-group\n",
    "2. start the producer above (So that data is pupmed in post the group creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect pyspark to kafka and start stream processing\n",
    "# https://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "conf = SparkConf().set(\"spark.jars\",\"/home/jovyan/work/spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar\")\n",
    "print(conf.toDebugString())\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc,60)\n",
    "kafkaStream = KafkaUtils.createStream(\n",
    "    ssc=ssc, \n",
    "    zkQuorum='zookeeper.1.252d8i3iyqh7keyobvpjj4dz8:2181', \n",
    "    groupId='test-consumer-group',#, #Consumer Group \n",
    "    topics={'test':1})\n",
    "lines = kafkaStream.map(lambda x: x[1])\n",
    "lines.pprint()\n",
    "ssc.start()  \n",
    "print(\"Going to wait termination\")\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow Model Serving with Docker**\n",
    "\n",
    "https://www.tensorflow.org/tfx/serving/docker\n",
    "\n",
    "Running an instance of TSX Server\n",
    "1. docker pull tensorflow/serving\n",
    "2. git clone https://github.com/tensorflow/serving\n",
    "3. docker run -t --rm -p 8501:8501 -v \"C:\\Users\\srmenon\\Documents\\Jarvis\\serving\\tensorflow_serving\\servables\\tensorflow\\testdata\\saved_model_half_plus_two_cpu:/models/half_plus_two\" -e MODEL_NAME=half_plus_two  tensorflow/serving &\n",
    "4. curl -d \"{\\\"instances\\\": [1.0, 2.0, 5.0]}\" -g -X POST http://localhost:8501/v1/models/half_plus_two:predict\n",
    "5. docker network connect kafka-net xenodochial_tesla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Code to call TSX\n",
    "import requests\n",
    "r = requests.post(\n",
    "    \"http://10.0.1.48:8501/v1/models/half_plus_two:predict\",\n",
    "    \"{\\\"instances\\\": [1.0, 2.0, 5.0]}\")\n",
    "# Am able to reach the server using ip address - but name resolution is failing\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark streaming from Kafka with calls to TSX\n",
    "# DStream transformations : http://sungsoo.github.io/2015/04/06/transformations-on-dstreams.html\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "conf = SparkConf().set(\"spark.jars\",\"/home/jovyan/work/spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar\")\n",
    "print(conf.toDebugString())\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc,60) # 60 is the batch interval :)\n",
    "kafkaStream = KafkaUtils.createStream(\n",
    "    ssc=ssc, \n",
    "    zkQuorum='zookeeper.1.252d8i3iyqh7keyobvpjj4dz8:2181', \n",
    "    groupId='test-consumer-group',#, #Consumer Group \n",
    "    topics={'test':1})\n",
    "lines = kafkaStream.map(lambda x: x[1])\n",
    "lines.pprint()\n",
    "lines = lines.map(lambda a: str(json.loads(a)[\"number\"]))\n",
    "lines.pprint()\n",
    "lines = lines.reduce(lambda v,agg: agg+\", \"+v)\n",
    "lines.pprint()\n",
    "lines = lines.map(lambda a: requests.post(\n",
    "        \"http://10.0.1.48:8501/v1/models/half_plus_two:predict\",\n",
    "        \"{\\\"instances\\\": [\"+a+\"]}\").text)\n",
    "lines.pprint()\n",
    "ssc.start()  \n",
    "print(\"Going to wait termination\")\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Redis Image and DB**\n",
    "\n",
    "https://koukia.ca/installing-redis-on-windows-using-docker-containers-7737d2ebc25e\n",
    "https://github.com/MicrosoftArchive/redis/releases\n",
    "\n",
    "1. docker pull redis\n",
    "2. docker run --name redis-store -d redis\n",
    "3. docker network connect kafka-net redis-store\n",
    "4. goto C:\\Program Files\\Redis\n",
    "5. redis-cli -h localhost/127.0.0.1 -p 6379\n",
    "6. set key value\n",
    "7. get key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Redis from Python\n",
    "# !pip install redis\n",
    "import redis\n",
    "r = redis.StrictRedis(host = 'redis-store', port = 6379)\n",
    "#r.set(\"Radhika\",100)\n",
    "print(r.get(\"171.5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Redis from within PySpark context\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import redis\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "def SetVal(x):\n",
    "    r = redis.StrictRedis(host = 'redis-store', port = 6379)\n",
    "    r.set(x,int(x))\n",
    "\n",
    "conf = SparkConf().set(\"spark.jars\",\"/home/jovyan/work/spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar\")\n",
    "print(conf.toDebugString())\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc,60) # 60 is the batch interval :)\n",
    "kafkaStream = KafkaUtils.createStream(\n",
    "    ssc=ssc, \n",
    "    zkQuorum='zookeeper.1.252d8i3iyqh7keyobvpjj4dz8:2181', \n",
    "    groupId='test-consumer-group',#, #Consumer Group \n",
    "    topics={'test':1})\n",
    "lines = kafkaStream.map(lambda x: x[1])\n",
    "lines.pprint()\n",
    "lines = lines.map(lambda a: str(json.loads(a)[\"number\"]))\n",
    "lines.pprint()\n",
    "lines = lines.reduce(lambda v,agg: agg+\", \"+v)\n",
    "lines.pprint()\n",
    "lines = lines.map(lambda a: requests.post(\n",
    "        \"http://10.0.1.48:8501/v1/models/half_plus_two:predict\",\n",
    "        \"{\\\"instances\\\": [\"+a+\"]}\").text)\n",
    "lines.pprint()\n",
    "lines = lines.flatMap(lambda x: json.loads(x)[\"predictions\"])\n",
    "lines.pprint()\n",
    "lines = lines.map(lambda x: SetVal(x))\n",
    "lines.pprint()\n",
    "\n",
    "ssc.start()  \n",
    "print(\"Going to wait termination\")\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
