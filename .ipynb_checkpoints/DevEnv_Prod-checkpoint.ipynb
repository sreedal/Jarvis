{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prod Version of DevEnv**\n",
    "\n",
    "**This notebook is run using nbconvert after devenv service starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install twitter\n",
    "!pip install feedparser\n",
    "!pip install schedule\n",
    "!pip install kafka-python\n",
    "!pip install redis\n",
    "!pip install uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps, loads, load\n",
    "from kafka import KafkaProducer\n",
    "from datetime import datetime, timedelta\n",
    "from feedparser import parse\n",
    "from urllib.request import urlopen\n",
    "from twitter import Twitter\n",
    "import schedule\n",
    "from datetime import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"MSFT\",\"AAPL\"]\n",
    "feeds = [\"http://feeds.reuters.com/reuters/businessNews\"]\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['broker:9092'],\n",
    "    api_version= (0,11),\n",
    "    value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "# Authenticate with Twitter & search\n",
    "# https://pypi.org/project/twitter/\n",
    "# https://developer.twitter.com/en/apps/17019807\n",
    "from twitter import *\n",
    "t = Twitter(auth=OAuth(consumer_key=\"nDwKHutvaskyGecZkCS1SiYY8\", consumer_secret=\"zlQdJIuFUGGglbMO2kYMk6oNLFztzipxhRANNz5q5gqfn1tGb1\", token=\"73568030-GqfoqkfWRhqhARS9uzz51vg1UGSV7AGRaOxDkAurZ\", token_secret=\"1kiHGfyDfO7gcaJbGFnr3lnLA3vXc8ti9hJhj1lnTTvEm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadStreams():\n",
    "    # Yahoo Finance Stream for Stock Price\n",
    "    f_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            y_finance = load(urlopen(\"https://query1.finance.yahoo.com/v7/finance/chart/\"+ticker+\"?range=1h&interval=1h\"))\n",
    "            meta = y_finance['chart']['result'][0]['meta']\n",
    "            timestamp = meta['regularMarketTime']\n",
    "            price = meta['regularMarketPrice']\n",
    "            f_data[ticker] = {\"Type\": \"Finance\", \"Ticker\": ticker, \"Timestamp\":timestamp, \"Price\":price}\n",
    "        except:\n",
    "            print(\"Exception in Download Stream for \"+ticker)\n",
    "    \n",
    "    # News Stream from Reuters for Business\n",
    "    feed_data = {}\n",
    "    for feed in feeds:\n",
    "        feed_data[feed] = [{\"Type\": \"News\", \"Timestamp\": datetime.timestamp(datetime.now()), \"Title\":l.title, \"Summary\":l.summary, \"Link\":l.link} for l in (parse(feed))[\"entries\"]]\n",
    "        # print([l.title for l in d[\"entries\"]])\n",
    "\n",
    "    return f_data, feed_data\n",
    "\n",
    "def UploadStreamsToKafka(f_data, feed_data, PopularityOnly=False):\n",
    "    if(not PopularityOnly):\n",
    "        for key in f_data.keys():\n",
    "            producer.send('finance_ticker', value=f_data[key])\n",
    "        for key in feed_data.keys():\n",
    "            for entry in feed_data[key]:\n",
    "                producer.send('news_feed', value=entry)\n",
    "    # Create Popularity Click Feed\n",
    "    for key in feed_data.keys():\n",
    "        for entry in feed_data[key]:\n",
    "            a = t.search.tweets(q=entry[\"Title\"])\n",
    "            for s in a['statuses']:\n",
    "                if(s['retweet_count']>0):\n",
    "                    producer.send('click',value={'Type': 'Click', 'User': 'generic_'+str(uuid.uuid4()), 'Timestamp': datetime.timestamp(datetime.now()), 'Title': entry[\"Title\"], 'Link': entry[\"Link\"] })\n",
    "    producer.flush()\n",
    "\n",
    "def HourlyJob():\n",
    "    # Check, Download & Parse each stream\n",
    "    f_data, feed_data = DownloadStreams()\n",
    "    # Upload cleaned events into Kafka Topics\n",
    "    UploadStreamsToKafka(f_data, feed_data)\n",
    "    print(\"Hourly Run Events Complete - Sleeping now\")\n",
    "\n",
    "def MinutelyJob():\n",
    "    f_data, feed_data = DownloadStreams()\n",
    "    UploadStreamsToKafka(f_data, feed_data, True)\n",
    "    print(\"Minutely Run Events Complete - Sleeping now\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HourlyJob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinutelyJob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.clear()\n",
    "HourlyJob()\n",
    "\n",
    "schedule.every(15).minutes.do(HourlyJob)\n",
    "schedule.every(1).minutes.do(MinutelyJob)\n",
    "starttime = datetime.now()\n",
    "endtime = starttime + timedelta(minutes=5)\n",
    "\n",
    "while True: #datetime.now()<endtime:\n",
    "    schedule.run_pending()\n",
    "    sleep(10)\n",
    "    \n",
    "schedule.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
