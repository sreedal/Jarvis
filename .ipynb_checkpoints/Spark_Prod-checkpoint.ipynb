{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.7/site-packages (1.4.7)\n",
      "Requirement already satisfied: redis in /opt/conda/lib/python3.7/site-packages (3.3.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import redis\n",
    "import pyspark\n",
    "import base64\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from datetime import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'\n",
    "conf = SparkConf().set(\"spark.jars\",\"/home/jovyan/work/spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc,60) # 60 is the batch interval :)\n",
    "debug = True\n",
    "saveToLocalDisk = True\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window,WindowSpec;\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType, MapType\n",
    "\n",
    "def get_bucket(rank):\n",
    "    return math.floor(rank/10)\n",
    "\n",
    "def get_json(summary, title, link, timestamp):\n",
    "    return {\"Summary\":summary, \"Title\":title, \"Link\":link, \"Timestamp\":timestamp}\n",
    "\n",
    "def get_unicode_escaped_string(text):\n",
    "    if(text is None):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return text.encode(\"ascii\").decode(\"unicode_escape\")\n",
    "\n",
    "def SetValHistory(x):\n",
    "    r = redis.StrictRedis(host = 'redis-store', port = 6379)\n",
    "    idVal = str(uuid.uuid4())\n",
    "    if (r.get('LatestNews') != None):\n",
    "        try:\n",
    "            batchSet = json.loads(r.get('LatestNews'))\n",
    "            if (\"id\" in batchSet):\n",
    "                r.set('LatestNews',\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\"+batchSet[\"id\"]+\"\\\"}\")\n",
    "                r.set(idVal,\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\"+batchSet[\"id\"]+\"\\\"}\")\n",
    "            else:\n",
    "                r.set('LatestNews',\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "                r.set(idVal,\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        except Exception:\n",
    "            r.set('LatestNews',\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "            r.set(idVal,\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "    else:\n",
    "        r.set('LatestNews',\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        r.set(idVal,\"{\\\"tasks\\\":\"+json.dumps(x)+\", \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        \n",
    "def SetVal(x):\n",
    "    r = redis.StrictRedis(host = 'redis-store', port = 6379)\n",
    "    idVal = str(uuid.uuid4())\n",
    "    news = r.get('LatestNews')\n",
    "    if news != None:\n",
    "        try:\n",
    "            batchSet = json.loads(news)\n",
    "            r.set('LatestNews',\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\"+batchSet[\"id\"]+\"\\\"}\")\n",
    "            r.set(idVal,\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\"+batchSet[\"id\"]+\"\\\"}\")\n",
    "        except Exception:\n",
    "            r.set('LatestNews',\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "            r.set(idVal,\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "    else:\n",
    "        #try:\n",
    "        #    past_data = spark.read.option(\"header\", \"false\").csv(\"/home/jovyan/work/NewsData.csv/*.csv\")\n",
    "        #    window=Window.orderBy(col(\"Timestamp\").desc());\n",
    "        #    df = past_data.toDF(\"Timestamp\",\"Title\",\"Link\",\"Summary1\");\n",
    "        #    ranked = df.withColumn(\"Rank\",dense_rank().over(window)).cache().drop_duplicates();\n",
    "        #    get_bucket_udf = udf(get_bucket, IntegerType())\n",
    "        #    get_json_udf = udf(get_json, MapType(StringType(),StringType()))\n",
    "        #    get_unicode_escaped_string_udf = udf(get_unicode_escaped_string, StringType())\n",
    "        #    branked = ranked.withColumn(\"Bucket\", get_bucket_udf('Rank')) \\\n",
    "        #                    .withColumn(\"Summary\", get_unicode_escaped_string_udf(\"Summary1\")) \\\n",
    "        #                    .withColumn(\"JSON\",get_json_udf(\"Summary\",\"Title\",\"Link\",\"Timestamp\")) \\\n",
    "        #                    .groupBy(\"Bucket\").agg(collect_list(\"JSON\").alias(\"Dictionary\"))\n",
    "        #    for val in branked.select(\"Dictionary\").rdd.collect():\n",
    "        #        SetValHistory([item for sublist in val for item in sublist])\n",
    "        #except Exception:\n",
    "        #    r.set('LatestNews',\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        #    r.set(idVal,\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        r.set('LatestNews',\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        r.set(idVal,\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "        \n",
    "def SaveToNewsFile(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF() \\\n",
    "        .write.save(\"/home/jovyan/work/NewsData.csv\",\n",
    "                    maxRecordsPerFile=1000, format=\"csv\", mode=\"append\") \n",
    "    \n",
    "def SaveToClickFile(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF() \\\n",
    "        .write.save(\"/home/jovyan/work/ClickData.csv\",\n",
    "                    maxRecordsPerFile=1000, format=\"csv\", mode=\"append\")\n",
    "    \n",
    "def SaveToFile(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF( [ \"Ticker\", \"Timestamp\", \"Price\" ] ) \\\n",
    "        .write.save(\"/home/jovyan/work/FinanceData.csv\",\n",
    "                    maxRecordsPerFile=1000, format=\"csv\", mode=\"append\") \n",
    "        \n",
    "def ConvertToTuple(a):\n",
    "    if not (type(a) is list): \n",
    "        return (a['Timestamp'],a['Title'])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to wait termination\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:46:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:46:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-01-05 17:46:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaStream = KafkaUtils.createStream(\n",
    "    ssc=ssc, \n",
    "    zkQuorum='zk-cs:2181', \n",
    "    #zkQuorum='zookeeper:2181', \n",
    "    groupId='test-consumer-group',#, #Consumer Group \n",
    "    topics={'finance_ticker':1,'news_feed':2, 'click':3})\n",
    "\n",
    "olines = kafkaStream.map(lambda x: x[1]).map(lambda a: json.loads(a))\n",
    "if(debug):\n",
    "    olines.pprint()\n",
    "if(saveToLocalDisk):\n",
    "    olines.filter(lambda a: ('Type' in a) & (a['Type']==\"Finance\")) \\\n",
    "    .map(lambda a: (a['Ticker'],a['Timestamp'],a['Price'])) \\\n",
    "    .foreachRDD(SaveToFile)\n",
    "    \n",
    "    olines.filter(lambda a: ('Type' in a) & (a['Type']==\"News\")) \\\n",
    "    .map(lambda a: (a['Timestamp'],a['Title'],a['Link'],str(a['Summary'].encode(\"unicode_escape\").decode(\"utf8\")))) \\\n",
    "    .foreachRDD(SaveToNewsFile)\n",
    "    \n",
    "    olines.filter(lambda a: ('Type' in a) & (a['Type']==\"Click\")) \\\n",
    "    .foreachRDD(SaveToClickFile)\n",
    "    \n",
    "lines = olines.filter(lambda a: ('Type' in a) & (a['Type']==\"Finance\")) \\\n",
    "        .map(lambda a: str(a['Price'])) \\\n",
    "        .reduce(lambda v,agg: agg+\", \"+v)\n",
    "\n",
    "if(debug):\n",
    "    lines.pprint()\n",
    "\n",
    "lines = olines.filter(lambda a: ('Type' in a) & (a['Type']==\"News\")) \\\n",
    "        .map(lambda a: json.dumps(a)).reduce(lambda v,agg: agg+\", \"+v) \\\n",
    "        .map(lambda x: SetVal(x))\n",
    "    \n",
    "if(debug):\n",
    "    lines.pprint()\n",
    "    \n",
    "ssc.start()  \n",
    "print(\"Going to wait termination\")\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.map(lambda a: requests.post(\n",
    "        \"http://serving:8501/v1/models/half_plus_two:predict\",\n",
    "        \"{\\\"instances\\\": [\"+a+\"]}\").text)\n",
    "if(debug):\n",
    "    lines.pprint()\n",
    "\n",
    "lines = lines.flatMap(lambda x: json.loads(x)[\"predictions\"])\n",
    "if(debug):\n",
    "    lines.pprint()\n",
    "\n",
    "lines = lines.map(lambda x: SetVal(x))\n",
    "if(debug):\n",
    "    lines.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Causes error due to mixing up Spark and SQL Contexts. Needs redesigning to support bootstrapping\n",
    "try:\n",
    "            past_data = spark.read.option(\"header\", \"false\").csv(\"/home/jovyan/work/NewsData.csv/*.csv\")\n",
    "            window=Window.orderBy(col(\"Timestamp\").desc());\n",
    "            df = past_data.toDF(\"Timestamp\",\"Title\",\"Link\",\"Summary\");\n",
    "            ranked = df.withColumn(\"Rank\",dense_rank().over(window)).cache().drop_duplicates();\n",
    "            get_bucket_udf = udf(get_bucket, IntegerType())\n",
    "            get_json_udf = udf(get_json, MapType(StringType(),StringType()))\n",
    "            get_unicode_escaped_string_udf = udf(get_unicode_escaped_string, StringType())\n",
    "            branked = ranked.withColumn(\"Bucket\", get_bucket_udf('Rank')) \\\n",
    "                            .withColumn(\"Summary\", get_unicode_escaped_string_udf(\"Summary1\")) \\\n",
    "                            .withColumn(\"JSON\",get_json_udf(\"Summary\",\"Title\",\"Link\",\"Timestamp\")) \\\n",
    "                            .groupBy(\"Bucket\").agg(collect_list(\"JSON\").alias(\"Dictionary\"))\n",
    "            for val in branked.select(\"Dictionary\").rdd.collect():\n",
    "                SetValHistory([item for sublist in val for item in sublist])\n",
    "        except Exception:\n",
    "            r.set('LatestNews',\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")\n",
    "            r.set(idVal,\"{\\\"tasks\\\":[\"+str(x)+\"], \\\"id\\\":\\\"\"+idVal+\"\\\", \\\"next\\\":\\\"\\\"}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
