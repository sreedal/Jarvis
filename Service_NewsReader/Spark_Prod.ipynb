{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.7/site-packages (1.4.7)\n",
      "Requirement already satisfied: redis in /opt/conda/lib/python3.7/site-packages (3.3.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import redis\n",
    "import pyspark\n",
    "import urllib\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[2] pyspark-shell'\n",
    "conf = SparkConf().set(\"spark.jars\",\"/home/jovyan/work/spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc,60) # 60 is the batch interval :)\n",
    "debug = True\n",
    "saveToLocalDisk = True\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetVal(x):\n",
    "    r = redis.StrictRedis(host = 'redis-store', port = 6379)\n",
    "    r.set('LatestNews',\"{\\\"tasks\\\":[\"+str(x)+\"]}\")\n",
    "        \n",
    "def SaveToNewsFile(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF( [ \"Timestamp\", \"Title\",\"Summary\",\"Link\" ] ) \\\n",
    "        .write.save(\"/home/jovyan/work/NewsData.csv\",\n",
    "                    maxRecordsPerFile=1000, format=\"csv\", mode=\"append\") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to wait termination\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:43:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:44:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:45:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:46:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:47:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:48:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:49:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:50:00\n",
      "-------------------------------------------\n",
      "None\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-12-07 23:51:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaStream = KafkaUtils.createStream(\n",
    "    ssc=ssc, \n",
    "    zkQuorum='zookeeper:2181', \n",
    "    groupId='test-consumer-group',#, #Consumer Group \n",
    "    topics={'news_feed':1})\n",
    "\n",
    "olines = kafkaStream.map(lambda x: x[1]).map(lambda a: json.loads(a))\n",
    "    \n",
    "olines = olines.filter(lambda a: ('Type' in a) & (a['Type']==\"News\"))\n",
    "    \n",
    "\n",
    "if(saveToLocalDisk):\n",
    "     olines.map(lambda a: (a['Timestamp'],a['Title'],a['Summary'],a['Link'])) \\\n",
    "        .foreachRDD(SaveToNewsFile)\n",
    "\n",
    "olines = olines.map(lambda a: json.dumps(a)).reduce(lambda v,agg: agg+\", \"+v)\n",
    "\n",
    "olines = olines.map(lambda x: SetVal(x))\n",
    "if(debug):\n",
    "    olines.pprint()\n",
    "    \n",
    "ssc.start()  \n",
    "print(\"Going to wait termination\")\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines.map(lambda a: requests.post(\n",
    "        \"http://serving:8501/v1/models/half_plus_two:predict\",\n",
    "        \"{\\\"instances\\\": [\"+a+\"]}\").text)\n",
    "if(debug):\n",
    "    lines.pprint()\n",
    "\n",
    "lines = lines.flatMap(lambda x: json.loads(x)[\"predictions\"])\n",
    "if(debug):\n",
    "    lines.pprint()\n",
    "\n",
    "lines = lines.map(lambda x: SetVal(x))\n",
    "if(debug):\n",
    "    lines.pprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
